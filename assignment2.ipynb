{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6c7c26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width\n",
      "0             5.1          3.5           1.4          0.2\n",
      "1             4.9          3.0           1.4          0.2\n",
      "2             4.7          3.2           1.3          0.2\n",
      "3             4.6          3.1           1.5          0.2\n",
      "4             5.0          3.6           1.4          0.2\n",
      "..            ...          ...           ...          ...\n",
      "145           6.7          3.0           5.2          2.3\n",
      "146           6.3          2.5           5.0          1.9\n",
      "147           6.5          3.0           5.2          2.0\n",
      "148           6.2          3.4           5.4          2.3\n",
      "149           5.9          3.0           5.1          1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "0         setosa\n",
      "1         setosa\n",
      "2         setosa\n",
      "3         setosa\n",
      "4         setosa\n",
      "         ...    \n",
      "145    virginica\n",
      "146    virginica\n",
      "147    virginica\n",
      "148    virginica\n",
      "149    virginica\n",
      "Name: species, Length: 150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# A. Read the iris flower dataset from the following URL: https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv (Links to an external site.) and assign it to a Pandas DataFrame as you learned in tutorial Lab2-3. \n",
    "\n",
    "# importing the libaries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# reading the iris flower dataset from the URL and assigning it the the Pandas DataFrame\n",
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\n",
    "\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "\n",
    "# because we have feature cols in parameter it doesn't save the labels to x\n",
    "X = iris_df[feature_cols] \n",
    "\n",
    "# saving the labels to y variable\n",
    "y = iris_df['species']\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53dbde2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# B. Split the dataset into testing and training sets with the following parameters: test_size=0.4, random_state=6\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4567e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'virginica' 'setosa' 'setosa' 'virginica' 'versicolor'\n",
      " 'virginica' 'setosa' 'virginica' 'versicolor' 'virginica' 'versicolor'\n",
      " 'virginica' 'virginica' 'versicolor' 'versicolor' 'virginica'\n",
      " 'versicolor' 'versicolor' 'setosa' 'setosa' 'virginica' 'setosa' 'setosa'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'virginica' 'setosa' 'versicolor'\n",
      " 'setosa' 'versicolor' 'setosa' 'setosa' 'versicolor' 'virginica'\n",
      " 'versicolor' 'virginica' 'versicolor' 'setosa' 'setosa' 'virginica'\n",
      " 'versicolor' 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa'\n",
      " 'setosa' 'versicolor' 'virginica' 'virginica' 'virginica' 'setosa'\n",
      " 'virginica' 'setosa' 'setosa' 'setosa' 'versicolor' 'virginica']\n",
      "         actual  prediction\n",
      "4        setosa      setosa\n",
      "116   virginica   virginica\n",
      "2        setosa      setosa\n",
      "23       setosa      setosa\n",
      "123   virginica   virginica\n",
      "96   versicolor  versicolor\n",
      "134   virginica   virginica\n",
      "39       setosa      setosa\n",
      "137   virginica   virginica\n",
      "53   versicolor  versicolor\n",
      "127   virginica   virginica\n",
      "81   versicolor  versicolor\n",
      "115   virginica   virginica\n",
      "135   virginica   virginica\n",
      "74   versicolor  versicolor\n",
      "119   virginica  versicolor\n",
      "105   virginica   virginica\n",
      "51   versicolor  versicolor\n",
      "92   versicolor  versicolor\n",
      "32       setosa      setosa\n",
      "37       setosa      setosa\n",
      "120   virginica   virginica\n",
      "44       setosa      setosa\n",
      "0        setosa      setosa\n",
      "55   versicolor  versicolor\n",
      "72   versicolor  versicolor\n",
      "87   versicolor  versicolor\n",
      "102   virginica   virginica\n",
      "30       setosa      setosa\n",
      "93   versicolor  versicolor\n",
      "45       setosa      setosa\n",
      "59   versicolor  versicolor\n",
      "16       setosa      setosa\n",
      "13       setosa      setosa\n",
      "133   virginica  versicolor\n",
      "128   virginica   virginica\n",
      "64   versicolor  versicolor\n",
      "146   virginica   virginica\n",
      "95   versicolor  versicolor\n",
      "49       setosa      setosa\n",
      "17       setosa      setosa\n",
      "103   virginica   virginica\n",
      "71   versicolor  versicolor\n",
      "61   versicolor  versicolor\n",
      "46       setosa      setosa\n",
      "12       setosa      setosa\n",
      "52   versicolor  versicolor\n",
      "27       setosa      setosa\n",
      "34       setosa      setosa\n",
      "54   versicolor  versicolor\n",
      "118   virginica   virginica\n",
      "117   virginica   virginica\n",
      "121   virginica   virginica\n",
      "6        setosa      setosa\n",
      "111   virginica   virginica\n",
      "18       setosa      setosa\n",
      "38       setosa      setosa\n",
      "20       setosa      setosa\n",
      "58   versicolor  versicolor\n",
      "108   virginica   virginica\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# C. Instantiate a KNN object with K=3, train it on the training set and test it on the testing set. Then, calculate the accuracy of your prediction as you learned in Lab3.\n",
    "k = 3\n",
    "iris_training_set = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# print out the prediction of the two testing data we used\n",
    "print(y_predict)\n",
    "\n",
    "\n",
    "# printing results\n",
    "results = pd.DataFrame()\n",
    "results['actual'] = y_test \n",
    "results['prediction'] = y_predict \n",
    "print(results)\n",
    "\n",
    "\n",
    "# importing the class that calculates the accuray score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "# printing accuray score\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46791284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  5 the accuracy is:  0.9833333333333333\n",
      "For k =  7 the accuracy is:  0.9666666666666667\n",
      "For k =  11 the accuracy is:  0.9666666666666667\n",
      "For k =  15 the accuracy is:  0.9333333333333333\n",
      "For k =  27 the accuracy is:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# D. Repeat part (c) for K=1, K=5, K=7, K=11, K=15, K=27, K=59 (you can simply use a “for loop,” and save the final accuracy results in a list). Does the accuracy always get better by increasing the value K?\n",
    "\n",
    "for k in range(2, 59):\n",
    "    if k == 1 or k == 5 or k == 7 or k == 11 or k == 15 or k == 27 or k == 59:\n",
    "        iris_training_set = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "        # training it on the training set\n",
    "        iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "        # testing on the testing set using the test data from b that we split.\n",
    "        y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "        # passing actual data and prediction data thru the class to get accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "        # printing accuray score\n",
    "        print(\"For k = \", k, \"the accuracy is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "84d0bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  ['petal_width']  k =  3 the accuracy is:  0.7166666666666667\n",
      "For  ['petal_width']  k =  3 the accuracy is:  0.5666666666666667\n",
      "For  ['petal_length']  k =  3 the accuracy is:  0.9333333333333333\n",
      "For  ['petal_width']  k =  3 the accuracy is:  0.95\n",
      "The best feature is petal_width with .95 accuracy and the second best feature is petal_lenght with .93 accuracy\n"
     ]
    }
   ],
   "source": [
    "# E. Now, suppose that we would like to make prediction based on only ONE single feature. To find the best single feature, we have to try every feature individually. In other word, we have to repeat part (c) with K=3, four times (each time using only one of the 4 features), and compute the accuracy each time. Then, check the accuracies. \n",
    "# Which individual feature provide the best accuracy (the best feature)?  Which one is the second best feature? (Note: you have to train, test, and evaluate your model 4 times, each time on a dataset including only one of the features, and save the final accuracy results in a list).\n",
    "\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "feature1 = ['sepal_length']\n",
    "feature2 = ['sepal_width']\n",
    "feature3 = ['petal_length']\n",
    "feature4 = ['petal_width']\n",
    "\n",
    "X1 = iris_df[feature1] \n",
    "X2 = iris_df[feature2] \n",
    "X3 = iris_df[feature3] \n",
    "X4 = iris_df[feature4] \n",
    "\n",
    "#defining my list to store accuracy\n",
    "accuracy_list=[]\n",
    "\n",
    "k=3\n",
    "iris_training_set = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "########################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "\n",
    "########################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "# printing accuray score\n",
    "print(\"For \", feature4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "\n",
    "########################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature3, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "\n",
    "########################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "\n",
    "print(\"The best feature is petal_width with .95 accuracy and the second best feature is petal_lenght with .93 accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ad81150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For  ['sepal_length', 'sepal_width']  k =  3 the accuracy is:  0.8166666666666667\n",
      "For  ['sepal_length', 'petal_length']  k =  3 the accuracy is:  0.9833333333333333\n",
      "For  ['sepal_length', 'petal_width']  k =  3 the accuracy is:  0.95\n",
      "For  ['sepal_width', 'petal_length']  k =  3 the accuracy is:  0.95\n",
      "For  ['sepal_width', 'petal_width']  k =  3 the accuracy is:  0.95\n",
      "For  ['petal_length', 'petal_width']  k =  3 the accuracy is:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# F. Now, we want to repeat part (e), this time using two features. you have to train, test, and evaluate your model for 6 different cases: using (1st and 2nd features), (1st and 3rd features), (1st and 4th  features), (2nd  and 3rd  features), (2nd and 4th features), (3rd and 4th  features)!    \n",
    "# Which “feature pair” provides the best accuracy?\n",
    "\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "feature1and2 = ['sepal_length', 'sepal_width']\n",
    "feature1and3 = ['sepal_length', 'petal_length']\n",
    "feature1and4 = ['sepal_length', 'petal_width']\n",
    "feature2and3 = ['sepal_width', 'petal_length']\n",
    "feature2and4 = ['sepal_width', 'petal_width']\n",
    "feature3and4 = ['petal_length', 'petal_width']\n",
    "\n",
    "\n",
    "X12 = iris_df[feature1and2] \n",
    "X13 = iris_df[feature1and3] \n",
    "X14 = iris_df[feature1and4] \n",
    "X23 = iris_df[feature2and3]\n",
    "X24 = iris_df[feature2and4] \n",
    "X34 = iris_df[feature3and4] \n",
    "\n",
    "#defining my list to store accuracy\n",
    "accuracy_list=[]\n",
    "\n",
    "k=3\n",
    "iris_training_set = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!\n",
    "\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X12, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature1and2, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X13, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature1and3, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X14, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature1and4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X23, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature2and3, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X24, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature2and4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "################################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X34, y, test_size=0.4, random_state=6) # We can fix the random_state for reproducibility!\n",
    "\n",
    "# training it on the training set\n",
    "iris_training_set.fit(X_train, y_train)\n",
    "\n",
    "# testing on the testing set using the test data from b that we split.\n",
    "y_predict = iris_training_set.predict(X_test)\n",
    "\n",
    "# passing actual data and prediction data thru the class to get accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "accuracy_list.append(accuracy)\n",
    "\n",
    "# printing accuray score\n",
    "print(\"For \", feature3and4, \" k = \", k, \"the accuracy is: \", accuracy)\n",
    "\n",
    "################################################################################################################################\n",
    "print(\"feature 3 and 4 as a pair creates the best accuracy, an accuracy of 0.9666\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e3866a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature pair from part F is also the best feature and second best feature from part E (petal_lenght & petal width)\n"
     ]
    }
   ],
   "source": [
    "# G. Big Question: Does the “best feature pair” from part (f) contain of both “first best feature” and “second best feature” from part (e)? In other word, can we conclude that the “best two features” for classification are the first best feature along with the second best feature together?\n",
    "\n",
    "print(\"The best feature pair from part (F) is also the best feature and second best feature from part (E) (petal_lenght & petal width)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8917a1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It makes sense that the two highest individual scores combined two make an average would also make it the highest scores if we were calculating averages for accuracy\n"
     ]
    }
   ],
   "source": [
    " # F. Optional Question: Justify your answer for part (g)! If yes, why?  If no, why not?\n",
    "\n",
    "print(\"It makes sense that the two highest individual scores combined two make an average would also make it the highest scores if we were calculating averages for accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00538938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
